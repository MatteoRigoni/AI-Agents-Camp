{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO97xeKgv4AiqsoeeDSM39j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Local Model Agent Example\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-WpVoMfa1qe"
      },
      "outputs": [],
      "source": [
        "#unsloth: to load advanced LLM models\n",
        "#transformers: to tokenize\n",
        "#accelerate e bitsandbytes: to optimize perfomance\n",
        "!pip install unsloth transformers accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel"
      ],
      "metadata": {
        "id": "MvFMwjgHgDv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load model and tokenizer\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/Meta-Llama-3.1-8B-Instruct\",\n",
        "    max_seq_length=8192, #max sequence length\n",
        "    load_in_4bit=True #load model with format 4 bit, to reduce memory\n",
        ")"
      ],
      "metadata": {
        "id": "Xc0dh1rebCaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TextStreamer from Hugging FAce, to show real time generated output\n",
        "from transformers import TextStreamer"
      ],
      "metadata": {
        "id": "CyWpJlv7cP-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Set chat template\n",
        "from unsloth.chat_templates import get_chat_template"
      ],
      "metadata": {
        "id": "BqHumBTLcfYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Configuring tokenizer and fields mapping\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template=\"llama-3.1\",\n",
        "    mapping={\"role\":\"role\", \"content\":\"content\", \"user\":\"human\", \"assistant\": \"gpt\"}\n",
        ")"
      ],
      "metadata": {
        "id": "i8hwdWxsczHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_streamer = TextStreamer(tokenizer) #to have stream real-time of output text"
      ],
      "metadata": {
        "id": "xr0Qcs5CdOAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Enable inference mode to speed up the model 2x\n",
        "FastLanguageModel.for_inference(model)"
      ],
      "metadata": {
        "id": "v_m0bVWZdT6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to send prompt to model via tokenizer\n",
        "def prompt_model(messages):\n",
        "  input_ids  = tokenizer.apply_chat_template(\n",
        "      messages,\n",
        "      tokenize=True,\n",
        "      add_generation_prompt=True,\n",
        "      return_tensors=\"pt\"\n",
        "  ).to(\"cuda\")\n",
        "\n",
        "  print(f\"Structure of inputs: {input_ids}\")\n",
        "  print(f\"Size of inputs: {input_ids.shape}\")\n",
        "\n",
        "  response = model.generate(\n",
        "      input_ids =input_ids,\n",
        "      streamer=text_streamer,\n",
        "      max_new_tokens=1024,\n",
        "      use_cache=True\n",
        "  )\n",
        "\n",
        "  #decode response\n",
        "  response_text = tokenizer.decode(response[0], skip_special_tokens=True)\n",
        "  print(f\"Response text: {response_text}\")\n",
        "\n",
        "  return response_text"
      ],
      "metadata": {
        "id": "dsoySHtCd2UW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to do the booking\n",
        "def interactive_booking():\n",
        "  print(\"Welcome to out booking system powered by Meta-Llama-3.1-8B-Instruct\")\n",
        "  chat_history=[]\n",
        "\n",
        "  while True:\n",
        "    request = input(\"Insert your booking request (or digit 'exit')\")\n",
        "    if request.lower() == \"exit\":\n",
        "      print(\"Thanks for using our system. Bye.\")\n",
        "      break\n",
        "\n",
        "    print(f\"\\n**Cliente:** \\\"{request}\\\"\")\n",
        "    chat_history.append({\"role\":\"human\", \"content\":request})\n",
        "\n",
        "    model_response = prompt_model(chat_history)\n",
        "    print(f\"\\n**Restaurant:** \\\"{model_response}\\\"\")\n",
        "    chat_history.append({\"role\":\"assistant\", \"content\":model_response})\n",
        "\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "_9YrMJR8fzYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "interactive_booking()"
      ],
      "metadata": {
        "id": "5tJkevXHkD4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6vX_NQC9kFAu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
