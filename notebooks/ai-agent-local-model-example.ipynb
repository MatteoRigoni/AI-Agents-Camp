{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Model Agent Example\n",
    "\n",
    "Demonstrates building a simple agent using a locally hosted language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q-WpVoMfa1qe"
   },
   "outputs": [],
   "source": [
    "#unsloth: to load advanced LLM models\n",
    "#transformers: to tokenize\n",
    "#accelerate e bitsandbytes: to optimize perfomance\n",
    "!pip install unsloth transformers accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MvFMwjgHgDv_"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xc0dh1rebCaT"
   },
   "outputs": [],
   "source": [
    "#Load model and tokenizer\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/Meta-Llama-3.1-8B-Instruct\",\n",
    "    max_seq_length=8192, #max sequence length\n",
    "    load_in_4bit=True #load model with format 4 bit, to reduce memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CyWpJlv7cP-q"
   },
   "outputs": [],
   "source": [
    "#TextStreamer from Hugging FAce, to show real time generated output\n",
    "from transformers import TextStreamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BqHumBTLcfYD"
   },
   "outputs": [],
   "source": [
    "#Set chat template\n",
    "from unsloth.chat_templates import get_chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i8hwdWxsczHn"
   },
   "outputs": [],
   "source": [
    "#Configuring tokenizer and fields mapping\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"llama-3.1\",\n",
    "    mapping={\"role\":\"role\", \"content\":\"content\", \"user\":\"human\", \"assistant\": \"gpt\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xr0Qcs5CdOAD"
   },
   "outputs": [],
   "source": [
    "text_streamer = TextStreamer(tokenizer) #to have stream real-time of output text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v_m0bVWZdT6Z"
   },
   "outputs": [],
   "source": [
    "#Enable inference mode to speed up the model 2x\n",
    "FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dsoySHtCd2UW"
   },
   "outputs": [],
   "source": [
    "#Function to send prompt to model via tokenizer\n",
    "def prompt_model(messages):\n",
    "  input_ids  = tokenizer.apply_chat_template(\n",
    "      messages,\n",
    "      tokenize=True,\n",
    "      add_generation_prompt=True,\n",
    "      return_tensors=\"pt\"\n",
    "  ).to(\"cuda\")\n",
    "\n",
    "  print(f\"Structure of inputs: {input_ids}\")\n",
    "  print(f\"Size of inputs: {input_ids.shape}\")\n",
    "\n",
    "  response = model.generate(\n",
    "      input_ids =input_ids,\n",
    "      streamer=text_streamer,\n",
    "      max_new_tokens=1024,\n",
    "      use_cache=True\n",
    "  )\n",
    "\n",
    "  #decode response\n",
    "  response_text = tokenizer.decode(response[0], skip_special_tokens=True)\n",
    "  print(f\"Response text: {response_text}\")\n",
    "\n",
    "  return response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_9YrMJR8fzYN"
   },
   "outputs": [],
   "source": [
    "#Function to do the booking\n",
    "def interactive_booking():\n",
    "  print(\"Welcome to out booking system powered by Meta-Llama-3.1-8B-Instruct\")\n",
    "  chat_history=[]\n",
    "\n",
    "  while True:\n",
    "    request = input(\"Insert your booking request (or digit 'exit')\")\n",
    "    if request.lower() == \"exit\":\n",
    "      print(\"Thanks for using our system. Bye.\")\n",
    "      break\n",
    "\n",
    "    print(f\"\\n**Cliente:** \\\"{request}\\\"\")\n",
    "    chat_history.append({\"role\":\"human\", \"content\":request})\n",
    "\n",
    "    model_response = prompt_model(chat_history)\n",
    "    print(f\"\\n**Restaurant:** \\\"{model_response}\\\"\")\n",
    "    chat_history.append({\"role\":\"assistant\", \"content\":model_response})\n",
    "\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5tJkevXHkD4m"
   },
   "outputs": [],
   "source": [
    "interactive_booking()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6vX_NQC9kFAu"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyO97xeKgv4AiqsoeeDSM39j",
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
