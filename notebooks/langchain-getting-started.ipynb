{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with LangChain\n",
    "\n",
    "Covers fundamental LangChain components for chaining LLM prompts and tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zgqmGYMEuv6s",
    "outputId": "9a756c24-a362-4023-c1a1-8b38a4d20610"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.27)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.74)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.9)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.4.13)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.43)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.14.1)\n",
      "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (3.11.1)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.8.3)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.10.0)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n",
      "Collecting python_dotenv\n",
      "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
      "Downloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: python_dotenv\n",
      "Successfully installed python_dotenv-1.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain\n",
    "!pip install -qU langchain-groq\n",
    "!pip install -qU langchain-openai\n",
    "!pip install python_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wGURRZiFlxnv"
   },
   "source": [
    "## Getting started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "b3ofefVVllLU"
   },
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g_c446xvlwsx",
    "outputId": "a29d30b9-bd4d-4017-a5d4-b35451a85187"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Wa3pq5TfmKQU"
   },
   "outputs": [],
   "source": [
    "gpt4o = ChatOpenAI(\n",
    "    api_key = os.environ.get(\"OPENAI_API_KEY\"),\n",
    "    model_name = \"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q1ih4UYPmJu-",
    "outputId": "da8ecb38-56f9-4577-a207-b45cc794580e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='La temperatura ideale per cuocere la pizza varia in base al tipo di forno che stai utilizzando. In generale:\\n\\n- **Forno di casa**: Se usi un forno tradizionale, è consigliabile preriscaldarlo a circa 250-280°C. Questo aiuta a ottenere una base croccante e una cottura uniforme.\\n  \\n- **Forno a legna**: Questi forni possono raggiungere temperature molto più elevate, intorno ai 400-500°C. La pizza in questo tipo di forno cuoce molto rapidamente, in soli 90 secondi a 3 minuti.\\n\\n- **Forno elettrico o a gas**: Anche in questo caso, è meglio impostare la temperatura su 250-300°C per risultati ottimali.\\n\\nRicorda di utilizzare una pietra refrattaria se possibile, poiché aiuta a mantenere e distribuire il calore in modo uniforme, rendendo la pizza ancora più deliziosa!' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 201, 'prompt_tokens': 38, 'total_tokens': 239, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'id': 'chatcmpl-C4j0j39TU4hlG9ljoddcBdx3s8MWt', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--f34935da-58f0-4269-ac09-005cda7c80c4-0' usage_metadata={'input_tokens': 38, 'output_tokens': 201, 'total_tokens': 239, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"Sei un chatbot assistente di cucina. Aiuta l'utente con le ricette.\",\n",
    "    ),\n",
    "    (\"human\", \"A che temperatura cuoce la pizza?\"),\n",
    "]\n",
    "\n",
    "gpt_ping = gpt4o.invoke(messages)\n",
    "print(gpt_ping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "44U2AuecmYtb"
   },
   "source": [
    "## Prompting Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hMeynwOnnF2x",
    "outputId": "2cbd6df0-1f0a-408e-8312-c5769b55c96c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='Dimmi tre razze appartenti a alla specie squali')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\"Dimmi tre razze appartenti a alla specie {specie}\")\n",
    "\n",
    "prompt_template.invoke({\"specie\": \"squali\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mDoxH_DlmQ7w",
    "outputId": "5aa1a80f-e304-43b5-be68-69a6bcd7164a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='Sei un chatbot esperto di zoologia', additional_kwargs={}, response_metadata={}), HumanMessage(content='Dimmi tre razze appartenti a alla specie dinosauri', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Sei un chatbot esperto di zoologia\"),\n",
    "    (\"user\", \"Dimmi tre razze appartenti a alla specie {specie}\")\n",
    "])\n",
    "\n",
    "prompt_template.invoke({\"specie\": \"dinosauri\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LB8hBLc1oCmL"
   },
   "source": [
    "## LangChain Expression Language (LCEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ir-vB1P-rfvF"
   },
   "source": [
    "Double prompt to improve response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-fH4rAJpmnsr",
    "outputId": "0d0662f4-1f49-4d4a-9750-8b4ba49a7722"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```text=' Grazie mille per il tuo feedback. Faremo il nostro meglio per migliorarci in futuro.\\n'```\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "\n",
    "chat_history = \"\"\"\n",
    "Chatbot: Durante il tuo soggiorno, quale è il servizio che ti è piaciuto meno?\n",
    "User: Piscina\n",
    "Chatbot: Come mai la piscina?\n",
    "User: Era molto piccola e troppo calda.\n",
    "\"\"\"\n",
    "\n",
    "standard_response = \"\"\" Grazie mille per il tuo feedback. Faremo il nostro meglio per migliorarci in futuro.\n",
    "\"\"\"\n",
    "\n",
    "PROMPT_INTERACTION_1 = \"\"\" Sei un chatbot assistente in un servizio clienti di un hotel. Stai avendo una conversazione con un cliente:\n",
    "```{chat_history}```.\n",
    "Genera una risposta appropriata per continuare la conversazione, adattando questa risposta standard:\n",
    "Risposta standard: ```{standard_response}```.\n",
    "\"\"\"\n",
    "\n",
    "PROMPT_REFINE_1 = \"\"\" Sei un chatbot assistente in un servizio clienti di un hotel. Stai avendo una conversazione con un cliente.\n",
    "La tua ultima risposta è stata questa ```{first_response}```.\n",
    "Elimina eventuali domande e assicurati che sia simile a questa risposta standard: ```{standard_response}```.\n",
    "\"\"\"\n",
    "\n",
    "prompt_start= PromptTemplate(input_variables=[\"chat_history\",\"standard_response\"], template=PROMPT_INTERACTION_1)\n",
    "prompt_clean_questions = PromptTemplate(input_variables=[\"first_response\",\"standard_response\"], template=PROMPT_REFINE_1)\n",
    "std_resp = PromptTemplate.from_template(standard_response)\n",
    "\n",
    "chain_one = prompt_start | gpt4o | StrOutputParser()\n",
    "chain_two = prompt_clean_questions | gpt4o | StrOutputParser()\n",
    "\n",
    "final_chain =({\"first_response\" : chain_one, \"standard_response\": std_resp}\n",
    "        | RunnablePassthrough.assign(final_response = chain_two))\n",
    "\n",
    "result = final_chain.invoke({\"chat_history\": chat_history, \"standard_response\": standard_response})\n",
    "\n",
    "print(result['final_response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3-Tlgnpbr-tz"
   },
   "source": [
    "Customizing focus of response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dIRhjzwhr81Y"
   },
   "outputs": [],
   "source": [
    "chat_history = \"\"\"\n",
    "Chatbot: Durante il tuo soggiorno, quale è il servizio che ti è piaciuto meno?\n",
    "User: Piscina\n",
    "Chatbot: Come mai?\n",
    "User: Era molto piccola e troppo calda.\n",
    "Chatbot: In che modo pensi potremmo migliorarci la prossima volta?\n",
    "User: Un servizio di prenotazioni aiuterebbe magari\n",
    "\"\"\"\n",
    "\n",
    "standard_response = \"\"\" Grazie per il tuo feedback, ottimo consiglio.\n",
    "Faremo il possibile per migliorare il servizio piscina, ci spiace non sia stato all'altezza delle tue aspettative! \"\"\"\n",
    "\n",
    "user_first_impression = \"\"\"Era molto piccola e troppo calda. \"\"\"\n",
    "user_goal = \"\"\"Un servizio di prenotazioni aiuterebbe magari\"\"\"\n",
    "\n",
    "PROMPT_INTERACTION_F = \"\"\"Sei un chatbot assistente che sta aiutando un cliente insoddisfatto riguardo a un servizio dell'hotel.\n",
    "Stai avendo una conversazione: ```{chat_history}```.\n",
    "Genera una risposta appropriata per continuare questa conversazione, adattando questa risposta standard.\n",
    "Risposta standard: ```{standard_response}```\n",
    "L'utente ha detto questo sull'esperienza: {user_first_impression}:\n",
    "L'utente ti ha detto che spera di vedere miglioramenti: ```{user_goal}```\n",
    "Scrivi solo la risposta.\n",
    "\"\"\"\n",
    "\n",
    "prompt_start= PromptTemplate(input_variables=[\"chat_history\",\"standard_response\", \"user_first_impression\", \"user_goal\"], template=PROMPT_INTERACTION_F)\n",
    "\n",
    "chain_one = prompt_start | gpt4o | StrOutputParser()\n",
    "\n",
    "result = chain_one.invoke({\"chat_history\": chat_history, \"standard_response\": standard_response,\n",
    "                           \"user_first_impression\": user_first_impression, \"user_goal\": user_goal })\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XNftk1eDs7DU"
   },
   "outputs": [],
   "source": [
    "\n",
    "chat_history = \"\"\"\n",
    "Chatbot: Durante il tuo soggiorno, quale è il servizio che ti è piaciuto meno?\n",
    "User: Piscina\n",
    "Chatbot: Come mai?\n",
    "User: Era molto piccola e troppo calda.\n",
    "Chatbot: In che modo pensi potremmo migliorarci la prossima volta?\n",
    "User: Un servizio di prenotazioni aiuterebbe magari\n",
    "\"\"\"\n",
    "\n",
    "user_first_impression = \"\"\"Era molto piccola e troppo calda. \"\"\"\n",
    "user_response = \"\"\"Un servizio di prenotazioni aiuterebbe magari\"\"\"\n",
    "\n",
    "\n",
    "PROMPT_GOAL = \"\"\"Sei un chatbot assistente che sta aiutando un cliente insoddisfatto riguardo a un servizio dell'hotel.\n",
    "Stai avendo una conversazione: ```{chat_history}```.\n",
    "Qual è lo scopo dell'utente? In che modo aiuterebbe a migliorare?\n",
    "```{user_response}```\n",
    "\"\"\"\n",
    "\n",
    "prompt_start= PromptTemplate(input_variables=[\"chat_history\",\"standard_response\", \"user_response\"], template=PROMPT_GOAL)\n",
    "\n",
    "chain_one = prompt_start | llama3 | StrOutputParser()\n",
    "\n",
    "user_goal = chain_one.invoke({\"chat_history\": chat_history, \"user_response\": user_response })\n",
    "\n",
    "print(user_goal)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP04SAnDM9hGZQevJcBRTeQ",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
