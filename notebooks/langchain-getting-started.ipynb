{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP04SAnDM9hGZQevJcBRTeQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Getting Started with LangChain\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgqmGYMEuv6s",
        "outputId": "9a756c24-a362-4023-c1a1-8b38a4d20610"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.27)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.74)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.9)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.4.13)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (3.11.1)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n",
            "Collecting python_dotenv\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Downloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: python_dotenv\n",
            "Successfully installed python_dotenv-1.1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain\n",
        "!pip install -qU langchain-groq\n",
        "!pip install -qU langchain-openai\n",
        "!pip install python_dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting started\n"
      ],
      "metadata": {
        "id": "wGURRZiFlxnv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "from langchain_openai import ChatOpenAI\n",
        "import os"
      ],
      "metadata": {
        "id": "b3ofefVVllLU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "load_dotenv()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_c446xvlwsx",
        "outputId": "a29d30b9-bd4d-4017-a5d4-b35451a85187"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpt4o = ChatOpenAI(\n",
        "    api_key = os.environ.get(\"OPENAI_API_KEY\"),\n",
        "    model_name = \"gpt-4o-mini\")"
      ],
      "metadata": {
        "id": "Wa3pq5TfmKQU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    (\n",
        "        \"system\",\n",
        "        \"Sei un chatbot assistente di cucina. Aiuta l'utente con le ricette.\",\n",
        "    ),\n",
        "    (\"human\", \"A che temperatura cuoce la pizza?\"),\n",
        "]\n",
        "\n",
        "gpt_ping = gpt4o.invoke(messages)\n",
        "print(gpt_ping)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1ih4UYPmJu-",
        "outputId": "da8ecb38-56f9-4577-a207-b45cc794580e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='La temperatura ideale per cuocere la pizza varia in base al tipo di forno che stai utilizzando. In generale:\\n\\n- **Forno di casa**: Se usi un forno tradizionale, è consigliabile preriscaldarlo a circa 250-280°C. Questo aiuta a ottenere una base croccante e una cottura uniforme.\\n  \\n- **Forno a legna**: Questi forni possono raggiungere temperature molto più elevate, intorno ai 400-500°C. La pizza in questo tipo di forno cuoce molto rapidamente, in soli 90 secondi a 3 minuti.\\n\\n- **Forno elettrico o a gas**: Anche in questo caso, è meglio impostare la temperatura su 250-300°C per risultati ottimali.\\n\\nRicorda di utilizzare una pietra refrattaria se possibile, poiché aiuta a mantenere e distribuire il calore in modo uniforme, rendendo la pizza ancora più deliziosa!' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 201, 'prompt_tokens': 38, 'total_tokens': 239, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'id': 'chatcmpl-C4j0j39TU4hlG9ljoddcBdx3s8MWt', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--f34935da-58f0-4269-ac09-005cda7c80c4-0' usage_metadata={'input_tokens': 38, 'output_tokens': 201, 'total_tokens': 239, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompting Template"
      ],
      "metadata": {
        "id": "44U2AuecmYtb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "prompt_template = PromptTemplate.from_template(\"Dimmi tre razze appartenti a alla specie {specie}\")\n",
        "\n",
        "prompt_template.invoke({\"specie\": \"squali\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMeynwOnnF2x",
        "outputId": "2cbd6df0-1f0a-408e-8312-c5769b55c96c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StringPromptValue(text='Dimmi tre razze appartenti a alla specie squali')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Sei un chatbot esperto di zoologia\"),\n",
        "    (\"user\", \"Dimmi tre razze appartenti a alla specie {specie}\")\n",
        "])\n",
        "\n",
        "prompt_template.invoke({\"specie\": \"dinosauri\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDoxH_DlmQ7w",
        "outputId": "5aa1a80f-e304-43b5-be68-69a6bcd7164a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "messages=[SystemMessage(content='Sei un chatbot esperto di zoologia', additional_kwargs={}, response_metadata={}), HumanMessage(content='Dimmi tre razze appartenti a alla specie dinosauri', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LangChain Expression Language (LCEL)\n"
      ],
      "metadata": {
        "id": "LB8hBLc1oCmL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Double prompt to improve response"
      ],
      "metadata": {
        "id": "Ir-vB1P-rfvF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import StrOutputParser\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "\n",
        "\n",
        "chat_history = \"\"\"\n",
        "Chatbot: Durante il tuo soggiorno, quale è il servizio che ti è piaciuto meno?\n",
        "User: Piscina\n",
        "Chatbot: Come mai la piscina?\n",
        "User: Era molto piccola e troppo calda.\n",
        "\"\"\"\n",
        "\n",
        "standard_response = \"\"\" Grazie mille per il tuo feedback. Faremo il nostro meglio per migliorarci in futuro.\n",
        "\"\"\"\n",
        "\n",
        "PROMPT_INTERACTION_1 = \"\"\" Sei un chatbot assistente in un servizio clienti di un hotel. Stai avendo una conversazione con un cliente:\n",
        "```{chat_history}```.\n",
        "Genera una risposta appropriata per continuare la conversazione, adattando questa risposta standard:\n",
        "Risposta standard: ```{standard_response}```.\n",
        "\"\"\"\n",
        "\n",
        "PROMPT_REFINE_1 = \"\"\" Sei un chatbot assistente in un servizio clienti di un hotel. Stai avendo una conversazione con un cliente.\n",
        "La tua ultima risposta è stata questa ```{first_response}```.\n",
        "Elimina eventuali domande e assicurati che sia simile a questa risposta standard: ```{standard_response}```.\n",
        "\"\"\"\n",
        "\n",
        "prompt_start= PromptTemplate(input_variables=[\"chat_history\",\"standard_response\"], template=PROMPT_INTERACTION_1)\n",
        "prompt_clean_questions = PromptTemplate(input_variables=[\"first_response\",\"standard_response\"], template=PROMPT_REFINE_1)\n",
        "std_resp = PromptTemplate.from_template(standard_response)\n",
        "\n",
        "chain_one = prompt_start | gpt4o | StrOutputParser()\n",
        "chain_two = prompt_clean_questions | gpt4o | StrOutputParser()\n",
        "\n",
        "final_chain =({\"first_response\" : chain_one, \"standard_response\": std_resp}\n",
        "        | RunnablePassthrough.assign(final_response = chain_two))\n",
        "\n",
        "result = final_chain.invoke({\"chat_history\": chat_history, \"standard_response\": standard_response})\n",
        "\n",
        "print(result['final_response'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fH4rAJpmnsr",
        "outputId": "0d0662f4-1f49-4d4a-9750-8b4ba49a7722"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```text=' Grazie mille per il tuo feedback. Faremo il nostro meglio per migliorarci in futuro.\\n'```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Customizing focus of response"
      ],
      "metadata": {
        "id": "3-Tlgnpbr-tz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history = \"\"\"\n",
        "Chatbot: Durante il tuo soggiorno, quale è il servizio che ti è piaciuto meno?\n",
        "User: Piscina\n",
        "Chatbot: Come mai?\n",
        "User: Era molto piccola e troppo calda.\n",
        "Chatbot: In che modo pensi potremmo migliorarci la prossima volta?\n",
        "User: Un servizio di prenotazioni aiuterebbe magari\n",
        "\"\"\"\n",
        "\n",
        "standard_response = \"\"\" Grazie per il tuo feedback, ottimo consiglio.\n",
        "Faremo il possibile per migliorare il servizio piscina, ci spiace non sia stato all'altezza delle tue aspettative! \"\"\"\n",
        "\n",
        "user_first_impression = \"\"\"Era molto piccola e troppo calda. \"\"\"\n",
        "user_goal = \"\"\"Un servizio di prenotazioni aiuterebbe magari\"\"\"\n",
        "\n",
        "PROMPT_INTERACTION_F = \"\"\"Sei un chatbot assistente che sta aiutando un cliente insoddisfatto riguardo a un servizio dell'hotel.\n",
        "Stai avendo una conversazione: ```{chat_history}```.\n",
        "Genera una risposta appropriata per continuare questa conversazione, adattando questa risposta standard.\n",
        "Risposta standard: ```{standard_response}```\n",
        "L'utente ha detto questo sull'esperienza: {user_first_impression}:\n",
        "L'utente ti ha detto che spera di vedere miglioramenti: ```{user_goal}```\n",
        "Scrivi solo la risposta.\n",
        "\"\"\"\n",
        "\n",
        "prompt_start= PromptTemplate(input_variables=[\"chat_history\",\"standard_response\", \"user_first_impression\", \"user_goal\"], template=PROMPT_INTERACTION_F)\n",
        "\n",
        "chain_one = prompt_start | gpt4o | StrOutputParser()\n",
        "\n",
        "result = chain_one.invoke({\"chat_history\": chat_history, \"standard_response\": standard_response,\n",
        "                           \"user_first_impression\": user_first_impression, \"user_goal\": user_goal })\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "id": "dIRhjzwhr81Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "chat_history = \"\"\"\n",
        "Chatbot: Durante il tuo soggiorno, quale è il servizio che ti è piaciuto meno?\n",
        "User: Piscina\n",
        "Chatbot: Come mai?\n",
        "User: Era molto piccola e troppo calda.\n",
        "Chatbot: In che modo pensi potremmo migliorarci la prossima volta?\n",
        "User: Un servizio di prenotazioni aiuterebbe magari\n",
        "\"\"\"\n",
        "\n",
        "user_first_impression = \"\"\"Era molto piccola e troppo calda. \"\"\"\n",
        "user_response = \"\"\"Un servizio di prenotazioni aiuterebbe magari\"\"\"\n",
        "\n",
        "\n",
        "PROMPT_GOAL = \"\"\"Sei un chatbot assistente che sta aiutando un cliente insoddisfatto riguardo a un servizio dell'hotel.\n",
        "Stai avendo una conversazione: ```{chat_history}```.\n",
        "Qual è lo scopo dell'utente? In che modo aiuterebbe a migliorare?\n",
        "```{user_response}```\n",
        "\"\"\"\n",
        "\n",
        "prompt_start= PromptTemplate(input_variables=[\"chat_history\",\"standard_response\", \"user_response\"], template=PROMPT_GOAL)\n",
        "\n",
        "chain_one = prompt_start | llama3 | StrOutputParser()\n",
        "\n",
        "user_goal = chain_one.invoke({\"chat_history\": chat_history, \"user_response\": user_response })\n",
        "\n",
        "print(user_goal)"
      ],
      "metadata": {
        "id": "XNftk1eDs7DU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
