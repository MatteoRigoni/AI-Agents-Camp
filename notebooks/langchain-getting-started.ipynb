{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa1fcdfe",
   "metadata": {},
   "source": [
    "# Langchain Getting Started\n",
    "\n",
    "This curated notebook demonstrates langchain getting started.\n",
    "\n",
    "## Contents\n",
    "1. Setup\n",
    "2. Tutorial\n",
    "3. Next Steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with LangChain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zgqmGYMEuv6s",
    "outputId": "9a756c24-a362-4023-c1a1-8b38a4d20610"
   },
   "outputs": [],
   "source": [
    "# TODO: describe cell\n",
    "!pip install langchain\n",
    "!pip install -qU langchain-groq\n",
    "!pip install -qU langchain-openai\n",
    "!pip install python_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wGURRZiFlxnv"
   },
   "source": [
    "## Getting started\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b3ofefVVllLU"
   },
   "outputs": [],
   "source": [
    "# TODO: describe cell\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g_c446xvlwsx",
    "outputId": "a29d30b9-bd4d-4017-a5d4-b35451a85187"
   },
   "outputs": [],
   "source": [
    "# TODO: describe cell\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wa3pq5TfmKQU"
   },
   "outputs": [],
   "source": [
    "# TODO: describe cell\n",
    "gpt4o = ChatOpenAI(\n",
    "    api_key = os.environ.get(\"OPENAI_API_KEY\"),\n",
    "    model_name = \"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q1ih4UYPmJu-",
    "outputId": "da8ecb38-56f9-4577-a207-b45cc794580e"
   },
   "outputs": [],
   "source": [
    "# TODO: describe cell\n",
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"Sei un chatbot assistente di cucina. Aiuta l'utente con le ricette.\",\n",
    "    ),\n",
    "    (\"human\", \"A che temperatura cuoce la pizza?\"),\n",
    "]\n",
    "\n",
    "gpt_ping = gpt4o.invoke(messages)\n",
    "print(gpt_ping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "44U2AuecmYtb"
   },
   "source": [
    "## Prompting Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hMeynwOnnF2x",
    "outputId": "2cbd6df0-1f0a-408e-8312-c5769b55c96c"
   },
   "outputs": [],
   "source": [
    "# TODO: describe cell\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\"Dimmi tre razze appartenti a alla specie {specie}\")\n",
    "\n",
    "prompt_template.invoke({\"specie\": \"squali\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mDoxH_DlmQ7w",
    "outputId": "5aa1a80f-e304-43b5-be68-69a6bcd7164a"
   },
   "outputs": [],
   "source": [
    "# TODO: describe cell\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Sei un chatbot esperto di zoologia\"),\n",
    "    (\"user\", \"Dimmi tre razze appartenti a alla specie {specie}\")\n",
    "])\n",
    "\n",
    "prompt_template.invoke({\"specie\": \"dinosauri\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LB8hBLc1oCmL"
   },
   "source": [
    "## LangChain Expression Language (LCEL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ir-vB1P-rfvF"
   },
   "source": [
    "Double prompt to improve response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-fH4rAJpmnsr",
    "outputId": "0d0662f4-1f49-4d4a-9750-8b4ba49a7722"
   },
   "outputs": [],
   "source": [
    "# TODO: describe cell\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "\n",
    "chat_history = \"\"\"\n",
    "Chatbot: Durante il tuo soggiorno, quale è il servizio che ti è piaciuto meno?\n",
    "User: Piscina\n",
    "Chatbot: Come mai la piscina?\n",
    "User: Era molto piccola e troppo calda.\n",
    "\"\"\"\n",
    "\n",
    "standard_response = \"\"\" Grazie mille per il tuo feedback. Faremo il nostro meglio per migliorarci in futuro.\n",
    "\"\"\"\n",
    "\n",
    "PROMPT_INTERACTION_1 = \"\"\" Sei un chatbot assistente in un servizio clienti di un hotel. Stai avendo una conversazione con un cliente:\n",
    "```{chat_history}```.\n",
    "Genera una risposta appropriata per continuare la conversazione, adattando questa risposta standard:\n",
    "Risposta standard: ```{standard_response}```.\n",
    "\"\"\"\n",
    "\n",
    "PROMPT_REFINE_1 = \"\"\" Sei un chatbot assistente in un servizio clienti di un hotel. Stai avendo una conversazione con un cliente.\n",
    "La tua ultima risposta è stata questa ```{first_response}```.\n",
    "Elimina eventuali domande e assicurati che sia simile a questa risposta standard: ```{standard_response}```.\n",
    "\"\"\"\n",
    "\n",
    "prompt_start= PromptTemplate(input_variables=[\"chat_history\",\"standard_response\"], template=PROMPT_INTERACTION_1)\n",
    "prompt_clean_questions = PromptTemplate(input_variables=[\"first_response\",\"standard_response\"], template=PROMPT_REFINE_1)\n",
    "std_resp = PromptTemplate.from_template(standard_response)\n",
    "\n",
    "chain_one = prompt_start | gpt4o | StrOutputParser()\n",
    "chain_two = prompt_clean_questions | gpt4o | StrOutputParser()\n",
    "\n",
    "final_chain =({\"first_response\" : chain_one, \"standard_response\": std_resp}\n",
    "        | RunnablePassthrough.assign(final_response = chain_two))\n",
    "\n",
    "result = final_chain.invoke({\"chat_history\": chat_history, \"standard_response\": standard_response})\n",
    "\n",
    "print(result['final_response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3-Tlgnpbr-tz"
   },
   "source": [
    "Customizing focus of response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dIRhjzwhr81Y"
   },
   "outputs": [],
   "source": [
    "# TODO: describe cell\n",
    "chat_history = \"\"\"\n",
    "Chatbot: Durante il tuo soggiorno, quale è il servizio che ti è piaciuto meno?\n",
    "User: Piscina\n",
    "Chatbot: Come mai?\n",
    "User: Era molto piccola e troppo calda.\n",
    "Chatbot: In che modo pensi potremmo migliorarci la prossima volta?\n",
    "User: Un servizio di prenotazioni aiuterebbe magari\n",
    "\"\"\"\n",
    "\n",
    "standard_response = \"\"\" Grazie per il tuo feedback, ottimo consiglio.\n",
    "Faremo il possibile per migliorare il servizio piscina, ci spiace non sia stato all'altezza delle tue aspettative! \"\"\"\n",
    "\n",
    "user_first_impression = \"\"\"Era molto piccola e troppo calda. \"\"\"\n",
    "user_goal = \"\"\"Un servizio di prenotazioni aiuterebbe magari\"\"\"\n",
    "\n",
    "PROMPT_INTERACTION_F = \"\"\"Sei un chatbot assistente che sta aiutando un cliente insoddisfatto riguardo a un servizio dell'hotel.\n",
    "Stai avendo una conversazione: ```{chat_history}```.\n",
    "Genera una risposta appropriata per continuare questa conversazione, adattando questa risposta standard.\n",
    "Risposta standard: ```{standard_response}```\n",
    "L'utente ha detto questo sull'esperienza: {user_first_impression}:\n",
    "L'utente ti ha detto che spera di vedere miglioramenti: ```{user_goal}```\n",
    "Scrivi solo la risposta.\n",
    "\"\"\"\n",
    "\n",
    "prompt_start= PromptTemplate(input_variables=[\"chat_history\",\"standard_response\", \"user_first_impression\", \"user_goal\"], template=PROMPT_INTERACTION_F)\n",
    "\n",
    "chain_one = prompt_start | gpt4o | StrOutputParser()\n",
    "\n",
    "result = chain_one.invoke({\"chat_history\": chat_history, \"standard_response\": standard_response,\n",
    "                           \"user_first_impression\": user_first_impression, \"user_goal\": user_goal })\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XNftk1eDs7DU"
   },
   "outputs": [],
   "source": [
    "# TODO: describe cell\n",
    "\n",
    "chat_history = \"\"\"\n",
    "Chatbot: Durante il tuo soggiorno, quale è il servizio che ti è piaciuto meno?\n",
    "User: Piscina\n",
    "Chatbot: Come mai?\n",
    "User: Era molto piccola e troppo calda.\n",
    "Chatbot: In che modo pensi potremmo migliorarci la prossima volta?\n",
    "User: Un servizio di prenotazioni aiuterebbe magari\n",
    "\"\"\"\n",
    "\n",
    "user_first_impression = \"\"\"Era molto piccola e troppo calda. \"\"\"\n",
    "user_response = \"\"\"Un servizio di prenotazioni aiuterebbe magari\"\"\"\n",
    "\n",
    "\n",
    "PROMPT_GOAL = \"\"\"Sei un chatbot assistente che sta aiutando un cliente insoddisfatto riguardo a un servizio dell'hotel.\n",
    "Stai avendo una conversazione: ```{chat_history}```.\n",
    "Qual è lo scopo dell'utente? In che modo aiuterebbe a migliorare?\n",
    "```{user_response}```\n",
    "\"\"\"\n",
    "\n",
    "prompt_start= PromptTemplate(input_variables=[\"chat_history\",\"standard_response\", \"user_response\"], template=PROMPT_GOAL)\n",
    "\n",
    "chain_one = prompt_start | llama3 | StrOutputParser()\n",
    "\n",
    "user_goal = chain_one.invoke({\"chat_history\": chat_history, \"user_response\": user_response })\n",
    "\n",
    "print(user_goal)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP04SAnDM9hGZQevJcBRTeQ",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
