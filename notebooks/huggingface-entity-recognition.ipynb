{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Recognition with Hugging Face\n",
    "\n",
    "Fine-tunes a Hugging Face model to identify named entities in text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sA8quPBPSqN2"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "from datasets import load_dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N9S2VluTTPEw"
   },
   "outputs": [],
   "source": [
    "#Loading model and relative tokenizer\n",
    "MODEL_NAME = \"dbmdz/bert-large-cased-finetuned-conll03-english\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForTokenClassification.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sZo0-x_HTdfI"
   },
   "outputs": [],
   "source": [
    "#Creating NER pipeline\n",
    "ner_pipeline = pipeline(task=\"token-classification\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uOKNIBu7Ua9_"
   },
   "outputs": [],
   "source": [
    "#Helper methods\n",
    "def analyze_text(text):\n",
    "  \"\"\"\n",
    "  Perform NER analysis on a text\n",
    "  Returs dataframe with results\n",
    "  \"\"\"\n",
    "  ner_results = ner_pipeline(text)\n",
    "  df = pd.DataFrame(ner_results)\n",
    "  df = df.rename(columns={\"word\": \"Parola\", \"entity_group\": \"Tipo\", \"score\": \"Confidenza\"})\n",
    "  return df\n",
    "\n",
    "def analyze_batch(sentences):\n",
    "  \"\"\"\n",
    "  Perform NER analysis on a batch of sentences\n",
    "  Returs dataframe with results\n",
    "  \"\"\"\n",
    "  batch_results = ner_pipeline(sentences)\n",
    "  results = []\n",
    "  for i, sentence_results in enumerate(batch_results):\n",
    "    for entity in sentence_results:\n",
    "      results.append({\n",
    "          \"Frase\": sentences[i],\n",
    "          \"Parola\": entity[\"word\"],\n",
    "          \"Tipo\": entity[\"entity_group\"],\n",
    "          \"Confidenza\": entity[\"score\"]\n",
    "      })\n",
    "  return pd.DataFrame(results)\n",
    "\n",
    "def save_results_to_csv(df, filename):\n",
    "  \"\"\"\n",
    "  Save results to CSV file\n",
    "  \"\"\"\n",
    "  df.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iIKYyMV8WMlg"
   },
   "outputs": [],
   "source": [
    "#Test (single)\n",
    "text = \"Juventus is an italian football team based in Turin\"\n",
    "\n",
    "#Analysis\n",
    "df_results = analyze_text(text)\n",
    "print(df_results)\n",
    "\n",
    "#Display recognized entities\n",
    "unique_entities = df_results[\"Tipo\"].unique()\n",
    "print(unique_entities)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNX0JHKNPKiA3uhvckQZKVp",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
